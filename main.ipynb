{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7d325aa-07a2-46c0-a91d-190751c50502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import utility, train, density_estimation, ood_detection, conf_calibration \n",
    "from utility import *\n",
    "from train import *\n",
    "from density_estimation import *\n",
    "from ood_detection import * \n",
    "from conf_calibration import *\n",
    "\n",
    "ID_dataset = \"CIFAR-10\"\n",
    "batch_size = 64\n",
    "val_size = 0.05\n",
    "val_seed = 99\n",
    "num_classes = 10\n",
    "embedding_dim = 512\n",
    "learning_rate = 1e-3\n",
    "dropout_rate = 0.5\n",
    "reg_param = 5e-2\n",
    "num_epochs = 100\n",
    "index = 0\n",
    "device = \"cuda:0\"\n",
    "ouput_dir = \"saved_results\"\n",
    "pretrained = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d7cadb8-ccd9-40bd-bc68-9f40bf30404c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./data\\test_32x32.mat\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainloader, validloader, testloader, ood_loader1, ood_loader2 = load_datasets(ID_dataset, batch_size, val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f060e9ae-4c97-4985-84d1-e6e367cd4f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function utility.resnet()>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b211e48-c05f-4c1c-ac49-1af3ba8da067",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "resnet() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#model = load_model(ID_dataset, pretrained, index, dropout_rate, device) \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model\n",
      "\u001b[1;31mTypeError\u001b[0m: resnet() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "#model = load_model(ID_dataset, pretrained, index, dropout_rate, device) \n",
    "model = resnet(num_classes)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afe9adfb-1d19-48b7-b9fa-0480ab34374f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for training & evaluation of the model\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  \n",
    "import  warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from itertools import cycle\n",
    "\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import dirichlet\n",
    "from scipy.special import gammaln\n",
    "from scipy.special import digamma\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.dirichlet import Dirichlet\n",
    "from torch.distributions.kl import kl_divergence as kl_div\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "def train_daedl(model, learning_rate, reg_param, num_epochs, trainloader, validloader, num_classes, device):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: 0.95 ** epoch)  \n",
    "    \n",
    "    VAL_ACC = []\n",
    "    VAL_LOSS = []\n",
    "    cnt = 0\n",
    "\n",
    "    model.to(device)        \n",
    "    model.train()\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        running_loss = 0.0\n",
    "          \n",
    "        for i, (x,y) in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "            x,y = x.to(device), y.to(device)\n",
    "          \n",
    "            alpha = 1e-6 + torch.exp(model(x))                                                         \n",
    "            alpha0 = alpha.sum(1).reshape(-1,1)\n",
    "            y_oh = F.one_hot(y, num_classes).to(device)\n",
    "            alpha_tilde = alpha * (1-y_oh) + y_oh\n",
    "                \n",
    "            expected_mse = torch.sum((y_oh - alpha / alpha0) ** 2 ) + torch.sum(((alpha * (alpha0 - alpha))) / ((alpha0 ** 2) * (alpha0 + 1)))                                                                           \n",
    "            kl_regularizer = kl_div(Dirichlet(1e-6 + alpha_tilde), Dirichlet(torch.ones_like(alpha_tilde))).sum() \n",
    "            loss = expected_mse + reg_param * kl_regularizer\n",
    "                \n",
    "            loss.backward()      \n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()    \n",
    "            \n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch % 20 == 0 and epoch > 0:\n",
    "        \n",
    "            total=0\n",
    "            correct=0\n",
    "            val_loss = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i, (x_v,y_v) in enumerate(validloader):\n",
    "                    x_v, y_v = x_v.to(device), y_v.to(device)\n",
    "\n",
    "                    alpha_v= torch.exp(model(x_v))\n",
    "                    alpha0_v = alpha_v.sum(1).reshape(-1,1)\n",
    "                    y_oh_v = F.one_hot(y_v, num_classes).to(device)  \n",
    "                    alpha_v_tilde = alpha_v * (1-y_oh_v) + y_oh_v\n",
    "                    \n",
    "                    expected_mse_v = torch.sum((y_oh_v - alpha_v/ alpha0_v) ** 2 ) + torch.sum(((alpha_v * (alpha0_v- alpha_v))) / ((alpha0_v ** 2) * (alpha0_v + 1)))\n",
    "                    kl_regularizer_v = kl_div(Dirichlet(alpha_v_tilde), Dirichlet(torch.ones_like(alpha_v_tilde))).sum()\n",
    "                    \n",
    "                    val_loss += expected_mse_v + reg_param * kl_regularizer_v               \n",
    "                    y_pred_v = alpha_v.argmax(1)\n",
    "                    \n",
    "                    total += y_v.size(0)\n",
    "                    correct += (y_pred_v == y_v).sum().item()\n",
    "\n",
    "            val_acc = 100*correct/total\n",
    "            VAL_LOSS.append(val_loss)\n",
    "            VAL_ACC.append(val_acc)\n",
    "            \n",
    "            if len(VAL_ACC) > 2 : \n",
    "                \n",
    "                r_acc = (VAL_ACC[-1] - VAL_ACC[-2]) / VAL_ACC[-2]\n",
    "                r_loss = (VAL_LOSS[-1] - VAL_LOSS[2]) / VAL_LOSS[-2]\n",
    "\n",
    "                if r_loss > -0.0001 :\n",
    "                    cnt = cnt + 1\n",
    "                else : \n",
    "                    cnt = 0\n",
    "                    \n",
    "            if cnt > 3 :\n",
    "                break\n",
    "                \n",
    "            print('Epoch {}, loss = {:.3f}'.format(epoch, val_loss)) \n",
    "            print('Validation Accuracy = {:.3f}'.format(val_acc))\n",
    "                 \n",
    "def eval_daedl(model, testloader, device):    \n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (x,y) in enumerate(testloader):\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            alpha_pred = torch.exp(model(x))\n",
    "            y_pred = alpha_pred.argmax(1)\n",
    "            \n",
    "            total += y.size(0)\n",
    "            correct += (y_pred == y).sum().item()\n",
    "            \n",
    "        test_acc = 100*correct/total\n",
    "        print(\"Test Accuracy:\",test_acc)\n",
    "    \n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7bf4f-3f29-42ae-8cdd-21decfce834f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Prepare the dataset\n",
    "X, y = make_moons(n_samples=1000, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Step 2: Define the Model\n",
    "class TwoMoonsClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoMoonsClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = TwoMoonsClassifier()\n",
    "\n",
    "# Step 3: Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Step 4: Training Loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss}\")\n",
    "\n",
    "# Step 5: Evaluate Model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "    print(f\"Accuracy on test set: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2237e1dd-ee50-40c0-b798-43cca2d51450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0199a07d-ecfe-4f6e-bbc2-889d41a717e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_daedl(model, learning_rate, reg_param, num_epochs, trainloader, validloader, num_classes, device)   \n",
    "test_acc = eval_daedl(model, testloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7060a1af-10bd-43f7-a0a4-8b3172186d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 743/743 [00:20<00:00, 36.69it/s]\n",
      "  0%|                                                                                          | 0/743 [00:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (100) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gda, p_z_train \u001b[38;5;241m=\u001b[39m \u001b[43mfit_gda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m ood_auroc, ood_aupr \u001b[38;5;241m=\u001b[39m ood_detection_daedl(model, gda, p_z_train, testloader, ood_loader1, ood_loader2, num_classes,                                              device)\n\u001b[0;32m      3\u001b[0m result \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Acc\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_acc,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOOD AUROC\u001b[39m\u001b[38;5;124m\"\u001b[39m: ood_auroc,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOOD AUPR\u001b[39m\u001b[38;5;124m\"\u001b[39m: ood_aupr}\n",
      "File \u001b[1;32m~\\[ICML 2024] DAEDL\\density_estimation.py:188\u001b[0m, in \u001b[0;36mfit_gda\u001b[1;34m(model, trainloader, num_classes, embedding_dim, device)\u001b[0m\n\u001b[0;32m    186\u001b[0m embeddings, labels \u001b[38;5;241m=\u001b[39m get_embeddings(model, trainloader, embedding_dim, torch\u001b[38;5;241m.\u001b[39mdouble, device, device) \n\u001b[0;32m    187\u001b[0m gda, jitter_eps \u001b[38;5;241m=\u001b[39m gmm_fit(embeddings, labels, num_classes)  \n\u001b[1;32m--> 188\u001b[0m train_log_probs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mgmm_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m p_z_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlogsumexp(train_log_probs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gda, p_z_train\n",
      "File \u001b[1;32m~\\[ICML 2024] DAEDL\\density_estimation.py:121\u001b[0m, in \u001b[0;36mgmm_evaluate\u001b[1;34m(net, gaussians_model, loader, device, num_classes, storage_device)\u001b[0m\n\u001b[0;32m    118\u001b[0m logit_B_C \u001b[38;5;241m=\u001b[39m gmm_forward(net, gaussians_model, data)\n\u001b[0;32m    120\u001b[0m end \u001b[38;5;241m=\u001b[39m start \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[1;32m--> 121\u001b[0m \u001b[43mlogits_N_C\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogit_B_C\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m labels_N[start:end]\u001b[38;5;241m.\u001b[39mcopy_(label, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    123\u001b[0m start \u001b[38;5;241m=\u001b[39m end\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (100) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "gda, p_z_train = fit_gda(model, trainloader, num_classes, embedding_dim, device)\n",
    "ood_auroc, ood_aupr = ood_detection_daedl(model, gda, p_z_train, testloader, ood_loader1, ood_loader2, num_classes,                                              device)\n",
    "result = {\"Test Acc\": test_acc,\"OOD AUROC\": ood_auroc,\"OOD AUPR\": ood_aupr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb6ecf6-8d7c-458b-85ed-c3f70532f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ID_dataset == \"CIFAR-10\" or \"CIFAR-100\":\n",
    "    brier, conf_aupr, conf_auroc = conf_calibration_daedl(model, gda, p_z_train, testloader, device)        \n",
    "    result[\"Conf AUROC\"] = conf_auroc\n",
    "    result[\"Conf AUPR\"] = conf_aupr\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "result_filepath = os.path.join(output_dir, 'results.json')\n",
    "\n",
    "with open(result_filepath, 'w') as result_file:\n",
    "    json.dump(result, result_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "888c0326-eabc-4bb1-a859-d3fcbc9c697a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./data\\test_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████▋                                                                      | 11/100 [09:01<1:16:59, 51.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, loss = 2500.173\n",
      "Validation Accuracy = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████▋                                                                      | 11/100 [09:33<1:17:19, 52.13s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m trainloader, validloader, testloader, ood_loader1, ood_loader2 \u001b[38;5;241m=\u001b[39m load_datasets(ID_dataset, batch_size, val_size)\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(ID_dataset, pretrained, index, dropout_rate, device)   \n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain_daedl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[0;32m      4\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m eval_daedl(model, testloader, device)  \n\u001b[0;32m      5\u001b[0m gda, p_z_train \u001b[38;5;241m=\u001b[39m fit_gda(model, trainloader, num_classes, embedding_dim, device)\n",
      "File \u001b[1;32m~\\[ICML 2024] DAEDL\\train.py:62\u001b[0m, in \u001b[0;36mtrain_daedl\u001b[1;34m(model, learning_rate, reg_param, num_epochs, trainloader, validloader, num_classes, device)\u001b[0m\n\u001b[0;32m     60\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()      \n\u001b[0;32m     61\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 62\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()    \n\u001b[0;32m     64\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainloader, validloader, testloader, ood_loader1, ood_loader2 = load_datasets(ID_dataset, batch_size, val_size)\n",
    "model = load_model(ID_dataset, pretrained, index, dropout_rate, device)   \n",
    "train_daedl(model, learning_rate, reg_param, num_epochs, trainloader, validloader, num_classes, device)   \n",
    "test_acc = eval_daedl(model, testloader, device)  \n",
    "gda, p_z_train = fit_gda(model, trainloader, num_classes, embedding_dim, device)\n",
    "ood_auroc, ood_aupr = ood_detection_daedl(model, gda, p_z_train, testloader, ood_loader1, ood_loader2, num_classes,                                              device)\n",
    "result = {\"Test Acc\": test_acc,\"OOD AUROC\": ood_auroc,\"OOD AUPR\": ood_aupr}\n",
    "    \n",
    "if ID_dataset == \"CIFAR-10\" or \"CIFAR-100\":\n",
    "    brier, conf_aupr, conf_auroc = conf_calibration_daedl(model, gda, p_z_train, testloader, device)        \n",
    "    result[\"Conf AUROC\"] = conf_auroc\n",
    "    result[\"Conf AUPR\"] = conf_aupr\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "result_filepath = os.path.join(output_dir, 'results.json')\n",
    "\n",
    "with open(result_filepath, 'w') as result_file:\n",
    "    json.dump(result, result_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
